{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1105996,"sourceType":"datasetVersion","datasetId":619369},{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceId":1926230,"sourceType":"datasetVersion","datasetId":1148896},{"sourceId":7685004,"sourceType":"datasetVersion","datasetId":4484220},{"sourceId":9652505,"sourceType":"datasetVersion","datasetId":5896023},{"sourceId":9656312,"sourceType":"datasetVersion","datasetId":5898896}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WEEK 7: VI. TEXT SUMMARIZATION\n## a. BASIC TEXT SUMMARIZATION USING TF-IDF AND COSINE SIMILARITY\n","metadata":{}},{"cell_type":"code","source":"# 1. Import Required Libraries\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Download necessary datasets for tokenization and stopwords\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# 2. Define Sample Text\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial\nintelligence concerned with the interactions between computers and human language, in\nparticular how to program computers to process and analyze large amounts of natural language\ndata.\nChallenges in natural language processing frequently involve speech recognition, natural\nlanguage understanding, and natural language generation.\n\"\"\"\n\n# 3. Preprocess the Text\n# Split the text into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Get the set of stopwords in English\nstop_words = set(stopwords.words('english'))\n\n# Function to preprocess each sentence by removing stopwords\ndef preprocess_sentence(sentence):\n    return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n\n# Preprocess all the sentences\npreprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n\n# 4. Compute TF-IDF Matrix\n# Create a TfidfVectorizer object\nvectorizer = TfidfVectorizer()\n\n# Transform the preprocessed sentences into TF-IDF features\ntfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n\n# 5. Compute Cosine Similarity\n# Compute cosine similarity between TF-IDF vectors of the sentences\ncosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# 6. Generate Summary\n# Function to generate a summary by ranking sentences based on their similarity scores\ndef generate_summary(sentences, sim_matrix, top_n=2):\n    # Compute the sum of similarity scores for each sentence\n    scores = sim_matrix.sum(axis=1)\n    \n    # Rank sentences based on the scores and select the top 'n' sentences\n    ranked_sentences = [sentences[i] for i in scores.argsort()[-top_n:]]\n    \n    # Return the summary as a string\n    return ' '.join(ranked_sentences)\n\n# Generate and print the summary\nsummary = generate_summary(sentences, cosine_sim_matrix)\nprint(\"Summary:\")\nprint(summary)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T15:04:18.650844Z","iopub.execute_input":"2024-10-17T15:04:18.651186Z","iopub.status.idle":"2024-10-17T15:04:20.554209Z","shell.execute_reply.started":"2024-10-17T15:04:18.651137Z","shell.execute_reply":"2024-10-17T15:04:20.553275Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nSummary:\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial\nintelligence concerned with the interactions between computers and human language, in\nparticular how to program computers to process and analyze large amounts of natural language\ndata. Challenges in natural language processing frequently involve speech recognition, natural\nlanguage understanding, and natural language generation.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. ABSTRACTIVE TEXT SUMMARIZATION WITH TRANSFORMERS","metadata":{}},{"cell_type":"code","source":"! pip install transformers datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:06:15.320483Z","iopub.execute_input":"2024-10-17T15:06:15.321035Z","iopub.status.idle":"2024-10-17T15:06:27.958500Z","shell.execute_reply.started":"2024-10-17T15:06:15.320993Z","shell.execute_reply":"2024-10-17T15:06:27.957527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# complte code 7 week 2 question\n\n# 1. Install required libraries (run this in your environment first)\n# !pip install transformers datasets\n\n# 2. Import Required Libraries\nfrom transformers import BartForConditionalGeneration, BartTokenizer\nfrom datasets import load_dataset\n\n# 3. Load the Dataset\n# Load the CNN/DailyMail dataset (test split, 1% for demonstration purposes)\ndataset = load_dataset('cnn_dailymail', '3.0.0', split='test[:1%]')\n\n# 4. Load Pre-trained BART Model and Tokenizer\n# Load pre-trained BART model and tokenizer\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# 5. Summarize Text\n# Function to summarize text\ndef summarize(text):\n    # Tokenize the input text\n    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    \n    # Generate the summary\n    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n    \n    # Decode the generated summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Sample Input and Output\n# Summarize a few sample articles from the dataset\nfor i in range(3):  # Loop through first 3 samples for demonstration\n    article = dataset[i]['article']\n    print(f\"Original Text {i+1}: {article}\\n\")\n    \n    # Generate and print the summary\n    summary = summarize(article)\n    print(f\"Summary {i+1}: {summary}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:06:48.950392Z","iopub.execute_input":"2024-10-17T15:06:48.951227Z","iopub.status.idle":"2024-10-17T15:07:43.569575Z","shell.execute_reply.started":"2024-10-17T15:06:48.951177Z","shell.execute_reply":"2024-10-17T15:07:43.568443Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f90b0b8bf454be781430f65ddbaec7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82fb6ad03fee4c12af5078e281174bae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93381338625b4e6c8612f1d5422defcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dca8c0bc56ea464cb9f9f61f2d891e8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ce9d1f284d4dd2a53a8e0cb150e76d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8083d3353dd4624b8671297ff8232f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04eba0818a5047439fcf1fc7c769a49f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cff9aaf0055441699a325b4e937bc869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14629931e5814a34897664ddc10374ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6d862de92554966942675a531f61224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3053c0fecc834338ae144fc5ae831bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe161c6c2c041098d6b6d6636b8a01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3432f4e96814af7b716d71bb6d9faa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e517cd5c45745beb440d46258c31621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1abaaa158ef4ddd83e539bc84b758b6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Original Text 1: (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n\nSummary 1: The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body.\n\nOriginal Text 2: (CNN)Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hammer in a misguided mercy killing and then buried in a field -- only to survive. That's according to Washington State University, where the dog -- a friendly white-and-black bully breed mix now named Theia -- has been receiving care at the Veterinary Teaching Hospital. Four days after her apparent death, the dog managed to stagger to a nearby farm, dirt-covered and emaciated, where she was found by a worker who took her to a vet for help. She was taken in by Moses Lake, Washington, resident Sara Mellado. \"Considering everything that she's been through, she's incredibly gentle and loving,\" Mellado said, according to WSU News. \"She's a true miracle dog and she deserves a good life.\" Theia is only one year old but the dog's brush with death did not leave her unscathed. She suffered a dislocated jaw, leg injuries and a caved-in sinus cavity -- and still requires surgery to help her breathe. The veterinary hospital's Good Samaritan Fund committee awarded some money to help pay for the dog's treatment, but Mellado has set up a fundraising page to help meet the remaining cost of the dog's care. She's also created a Facebook page to keep supporters updated. Donors have already surpassed the $10,000 target, inspired by Theia's tale of survival against the odds. On the fundraising page, Mellado writes, \"She is in desperate need of extensive medical procedures to fix her nasal damage and reset her jaw. I agreed to foster her until she finally found a loving home.\" She is dedicated to making sure Theia gets the medical attention she needs, Mellado adds, and wants to \"make sure she gets placed in a family where this will never happen to her again!\" Any additional funds raised will be \"paid forward\" to help other animals. Theia is not the only animal to apparently rise from the grave in recent weeks. A cat in Tampa, Florida, found seemingly dead after he was hit by a car in January, showed up alive in a neighbor's yard five days after he was buried by his owner. The cat was in bad shape, with maggots covering open wounds on his body and a ruined left eye, but remarkably survived with the help of treatment from the Humane Society.\n\nSummary 2: Theia, a one-year-old bully breed mix, was hit by a car and buried in a field. She managed to stagger to a nearby farm, dirt-covered and emaciated. She suffered a dislocated jaw, leg injuries and a caved-in sinus cavity -- and still requires surgery to help her breathe.\n\nOriginal Text 3: (CNN)If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U.S. Secretary of State John Kerry's opposite number in securing a breakthrough in nuclear discussions that could lead to an end to sanctions against Iran -- if the details can be worked out in the coming weeks. And he received a hero's welcome as he arrived in Iran on a sunny Friday morning. \"Long live Zarif,\" crowds chanted as his car rolled slowly down the packed street. You may well have read that he is \"polished\" and, unusually for one burdened with such weighty issues, \"jovial.\" An Internet search for \"Mohammad Javad Zarif\" and \"jovial\" yields thousands of results. He certainly has gone a long way to bring Iran in from the cold and allow it to rejoin the international community. But there are some facts about Zarif that are less well-known. Here are six: . In September 2013, Zarif tweeted \"Happy Rosh Hashanah,\" referring to the Jewish New Year. That prompted Christine Pelosi, the daughter of House Minority Leader Nancy Pelosi, to respond with a tweet of her own: \"Thanks. The New Year would be even sweeter if you would end Iran's Holocaust denial, sir.\" And, perhaps to her surprise, Pelosi got a response. \"Iran never denied it,\" Zarif tweeted back. \"The man who was perceived to be denying it is now gone. Happy New Year.\" The reference was likely to former Iranian President Mahmoud Ahmadinejad, who had left office the previous month. Zarif was nominated to be foreign minister by Ahmadinejad's successor, Hassan Rouhami. His foreign ministry notes, perhaps defensively, that \"due to the political and security conditions of the time, he decided to continue his education in the United States.\" That is another way of saying that he was outside the country during the demonstrations against the Shah of Iran, which began in 1977, and during the Iranian Revolution, which drove the shah from power in 1979. Zarif left the country in 1977, received his undergraduate degree from San Francisco State University in 1981, his master's in international relations from the University of Denver in 1984 and his doctorate from the University of Denver in 1988. Both of his children were born in the United States. The website of the Iranian Foreign Ministry, which Zarif runs, cannot even agree with itself on when he was born. The first sentence of his official biography, perhaps in a nod to the powers that be in Tehran, says Zarif was \"born to a religious traditional family in Tehran in 1959.\" Later on the same page, however, his date of birth is listed as January 8, 1960. And the Iranian Diplomacy website says he was born in in 1961 . So he is 54, 55 or maybe even 56. Whichever, he is still considerably younger than his opposite number, Kerry, who is 71. The feds investigated him over his alleged role in controlling the Alavi Foundation, a charitable organization. The U.S. Justice Department said the organization was secretly run on behalf of the Iranian government to launder money and get around U.S. sanctions. But last year, a settlement in the case, under which the foundation agreed to give a 36-story building in Manhattan along with other properties to the U.S. government, did not mention Zarif's name. Early in the Iranian Revolution, Zarif was among the students who took over the Iranian Consulate in San Francisco. The aim, says the website Iranian.com -- which cites Zarif's memoirs, titled \"Mr. Ambassador\" -- was to expel from the consulate people who were not sufficiently Islamic. Later, the website says, Zarif went to make a similar protest at the Iranian mission to the United Nations. In response, the Iranian ambassador to the United Nations offered him a job. In fact, he has now spent more time with Kerry than any other foreign minister in the world. And that amount of quality time will only increase as the two men, with help from other foreign ministers as well, try to meet a June 30 deadline for nailing down the details of the agreement they managed to outline this week in Switzerland.\n\nSummary 3: Mohammad Javad Zarif is the Iranian foreign minister. He has been John Kerry's opposite number in securing a breakthrough in nuclear discussions. But there are some facts about Zarif that are less well-known.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# WEEK 8: VII. TEXT ENTAILMENT APPLICATIONS IN PYTHON\n## a. BASIC TEXT ENTAILMENT USING SIMPLE RULE-BASED METHODS","metadata":{}},{"cell_type":"code","source":"# 1. Import necessary libraries and load dataset\nfrom datasets import load_dataset\nimport pandas as pd\nimport nltk\nfrom sklearn.metrics import accuracy_score\n\n# Download NLTK tokenizers\nnltk.download('punkt')\n\n# Sample dataset (as CNN/DailyMail isn't well-suited for entailment tasks, we'll create sample pairs)\ndata = pd.DataFrame({\n    'sentence1': [\"The cat is on the mat.\", \"The sun is shining brightly.\", \"The game is over.\"],\n    'sentence2': [\"The mat has a cat.\", \"The sky is bright.\", \"The players are done playing.\"],\n    'label': [False, True, True]  # Labels for entailment (True/False)\n})\n\n# 2. Preprocess the data: tokenize and convert to lowercase\ndef preprocess(text):\n    return nltk.word_tokenize(text.lower())\n\n# Apply preprocessing to both sentences\ndata['sentence1_tokens'] = data['sentence1'].apply(preprocess)\ndata['sentence2_tokens'] = data['sentence2'].apply(preprocess)\n\n# 3. Define simple rule-based method for text entailment\ndef simple_rule_based_entailment(s1, s2):\n    return set(s2).issubset(set(s1))\n\n# Apply the rule-based entailment check\ndata['prediction'] = data.apply(lambda row: simple_rule_based_entailment(row['sentence1_tokens'], row['sentence2_tokens']), axis=1)\n\n# 4. Evaluate the model\naccuracy = accuracy_score(data['label'], data['prediction'])\nprint(f'Accuracy: {accuracy}')\n\n# Output the data for reference\nprint(data[['sentence1', 'sentence2', 'label', 'prediction']])","metadata":{"execution":{"iopub.status.busy":"2024-10-17T15:08:08.357457Z","iopub.execute_input":"2024-10-17T15:08:08.358098Z","iopub.status.idle":"2024-10-17T15:08:08.385522Z","shell.execute_reply.started":"2024-10-17T15:08:08.358058Z","shell.execute_reply":"2024-10-17T15:08:08.384541Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.3333333333333333\n                      sentence1                      sentence2  label  \\\n0        The cat is on the mat.             The mat has a cat.  False   \n1  The sun is shining brightly.             The sky is bright.   True   \n2             The game is over.  The players are done playing.   True   \n\n   prediction  \n0       False  \n1       False  \n2       False  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b.NATURAL LANGUAGE INFERENCE WITH BERT","metadata":{}},{"cell_type":"code","source":"# Step 1: Import Required Libraries\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\n\n# Step 2: Load the Dataset\ndataset = load_dataset('snli')\n\n# Check the first few examples to understand the structure\nprint(dataset['train'].features)  # Check the features of the training dataset\nprint(dataset['train'][0:5])       # Print the first 5 examples from the training dataset\n\n# Step 3: Preprocess the Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_function(examples):\n    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n\n# Apply preprocessing to the dataset (train, validation, and test splits)\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n\n# Check the structure of the dataset again\nprint(encoded_dataset)\n\n# Step 4: Inspect the label column directly to understand its structure\nprint(\"Label examples:\")\nprint(encoded_dataset['train']['label'][0:5])  # Print the first 5 labels\n\n# Step 5: Identify unique labels\nunique_labels = set(encoded_dataset['train']['label'])\nprint(f\"Unique labels in the dataset: {unique_labels}\")\n\n# Step 6: Define label mapping and handle unexpected labels\nlabel_dict = {0: 0, 1: 1, 2: 2}  # Adjust this as necessary based on your labels\n\n# Step 7: Map the labels correctly, handle unexpected labels\ndef map_labels(example):\n    # Use the label_dict for mapping, and set a default for unexpected labels\n    label = example['label']\n    return {'labels': label_dict.get(label, -1)}  # Map to -1 if the label is unexpected\n\nencoded_dataset = encoded_dataset.map(map_labels)\n\n# Set the format for PyTorch\nencoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Step 8: Load the Pre-Trained BERT Model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n# Step 9: Set Up Training Arguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir='./results',          # Output directory\n    evaluation_strategy='epoch',     # Evaluation during each epoch\n    per_device_train_batch_size=16,  # Batch size for training\n    per_device_eval_batch_size=16,   # Batch size for evaluation\n    num_train_epochs=3,              # Number of training epochs\n    weight_decay=0.01,               # Strength of L2 regularization\n    logging_dir='./logs',            # Directory for logs\n)\n\n# Initialize the Trainer with the model, training arguments, and datasets\ntrainer = Trainer(\n    model=model,                         # The BERT model for training\n    args=training_args,                  # Training arguments\n    train_dataset=encoded_dataset['train'],  # Training dataset\n    eval_dataset=encoded_dataset['validation'],  # Validation dataset\n)\n\n# Step 10: Train the Model\ntrainer.train()\n\n# Step 11: Evaluate the Model\neval_results = trainer.evaluate()\nprint(f\"Evaluation Results: {eval_results}\")\n\n# Step 12: Make Predictions\npremise = \"A man inspects the uniform of a figure in some East Asian country.\"\nhypothesis = \"The man is sleeping.\"\n\n# Tokenize the input example\ninputs = tokenizer(premise, hypothesis, return_tensors='pt', padding=True, truncation=True, max_length=128)\n\n# Get model prediction\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_label = torch.argmax(outputs.logits).item()\n\n# Convert prediction to human-readable label\nlabel_map = {0: 'entailment', 1: 'contradiction', 2: 'neutral'}\nprint(f\"Predicted Label: {label_map[predicted_label]}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:23:49.932499Z","iopub.execute_input":"2024-10-17T16:23:49.933031Z","iopub.status.idle":"2024-10-17T16:23:55.864431Z","shell.execute_reply.started":"2024-10-17T16:23:49.932974Z","shell.execute_reply":"2024-10-17T16:23:55.862493Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}\n{'premise': ['A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'Children smiling and waving at camera', 'Children smiling and waving at camera'], 'hypothesis': ['A person is training his horse for a competition.', 'A person is at a diner, ordering an omelette.', 'A person is outdoors, on a horse.', 'They are smiling at their parents', 'There are children present'], 'label': [1, 2, 0, 1, 0]}\nDatasetDict({\n    test: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 10000\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 550152\n    })\n})\nLabel examples:\n[1, 2, 0, 1, 0]\nUnique labels in the dataset: {0, 1, 2, -1}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 62\u001b[0m\n\u001b[1;32m     51\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     52\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# Output directory\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m,     \u001b[38;5;66;03m# Evaluation during each epoch\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Directory for logs\u001b[39;00m\n\u001b[1;32m     59\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer with the model, training arguments, and datasets\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# The BERT model for training\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Training arguments\u001b[39;49;00m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Training dataset\u001b[39;49;00m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Validation dataset\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Step 10: Train the Model\u001b[39;00m\n\u001b[1;32m     70\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:406\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Seed must be set before instantiating the model when using model\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m enable_full_determinism(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mseed) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mfull_determinism \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mset_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_utils.py:105\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed, deterministic)\u001b[0m\n\u001b[1;32m    103\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 105\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    125\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 127\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:244\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/random.py:125\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    124\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"## WEEK 9: VIII. WORD AND SENTENCE EMBEDDING \n## a. BASIC WORD EMBEDDINGS WITH TF-IDF","metadata":{}},{"cell_type":"code","source":"#a\n# Step 1: Import Required Libraries\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Step 2: Load the Dataset\nnewsgroups = fetch_20newsgroups(subset='train')\ntexts = newsgroups.data  # Extract the document texts\n\n# Step 3: Preprocess the Text Data\nvectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\nX_tfidf = vectorizer.fit_transform(texts)  # Fit and transform the text data\n\n# Step 4: Explore the TF-IDF Matrix\nprint(\"TF-IDF matrix shape:\", X_tfidf.shape)  # Display shape of the matrix\nX_dense = X_tfidf.todense()  # Convert to dense format for better visualization\nprint(X_dense[0])  # Print the first document's TF-IDF vector\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:24:19.306598Z","iopub.execute_input":"2024-10-17T16:24:19.307032Z","iopub.status.idle":"2024-10-17T16:24:34.352256Z","shell.execute_reply.started":"2024-10-17T16:24:19.306997Z","shell.execute_reply":"2024-10-17T16:24:34.351207Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"TF-IDF matrix shape: (11314, 1000)\n[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.12190754 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.16779786 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.13299938 0.         0.         0.         0.73410701\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.15018239 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.12747266 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.16271777 0.         0.\n  0.         0.         0.         0.10513674 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.18701637 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.16060438 0.         0.         0.         0.\n  0.         0.06818803 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.14078947 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.08240921 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.03699896 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.17395851\n  0.13299938 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.1153094\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.17026123 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.06867111\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.0383646  0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.06636009 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.10908571 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.15730232\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.16291604 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.14488165 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.03687817\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.10609305 0.         0.11962569 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.07139339 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.11670224 0.         0.         0.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. GENERATING WORD EMBEDDINGS USING WORD2VEC AND GLOVE","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport re\n\n# Load a sample corpus (For demonstration, we'll use some random sentences)\ncorpus = [\n    \"This is a sample document.\",\n    \"Another example document.\",\n    \"Word embeddings capture semantic relationships.\",\n    \"GloVe and Word2Vec are popular embedding methods.\"\n]\n\n# Preprocess the text data (Tokenize and remove stop words)\nstop_words = stopwords.words('english')\n\ndef preprocess(text):\n    # Remove special characters, convert to lowercase, and tokenize\n    return [word for word in simple_preprocess(text) if word not in stop_words]\n\n# Apply preprocessing to the corpus\ntokenized_corpus = [preprocess(doc) for doc in corpus]\n\n# Display tokenized text\nprint(tokenized_corpus)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:24:48.041707Z","iopub.execute_input":"2024-10-17T16:24:48.042213Z","iopub.status.idle":"2024-10-17T16:25:20.709162Z","shell.execute_reply.started":"2024-10-17T16:24:48.042163Z","shell.execute_reply":"2024-10-17T16:25:20.708240Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[['sample', 'document'], ['another', 'example', 'document'], ['word', 'embeddings', 'capture', 'semantic', 'relationships'], ['glove', 'word', 'vec', 'popular', 'embedding', 'methods']]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train word2vex model\n\n# Import necessary library\nfrom gensim.models import Word2Vec\n\n# Train Word2Vec model (Skip-gram model, vector_size=100, window=5)\nword2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1)\n\n# Display the vector for a sample word (e.g., \"document\")\nword_vector = word2vec_model.wv['document']\nprint(f\"Word2Vec vector for 'document': {word_vector}\")\n\n# Find similar words to 'document'\nsimilar_words = word2vec_model.wv.most_similar('document')\nprint(f\"Words similar to 'document': {similar_words}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:25:29.033769Z","iopub.execute_input":"2024-10-17T16:25:29.034636Z","iopub.status.idle":"2024-10-17T16:25:29.054791Z","shell.execute_reply.started":"2024-10-17T16:25:29.034592Z","shell.execute_reply":"2024-10-17T16:25:29.053688Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Word2Vec vector for 'document': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\nWords similar to 'document': [('semantic', 0.21617142856121063), ('another', 0.09310110658407211), ('glove', 0.09291722625494003), ('example', 0.07963486760854721), ('embeddings', 0.06285078823566437), ('capture', 0.0270574688911438), ('relationships', 0.016134677454829216), ('word', -0.010839167051017284), ('methods', -0.027750369161367416), ('sample', -0.04125341773033142)]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Load pre-trained GloVe vectors (assuming 'glove.6B.100d.txt' is downloaded and available)\ndef load_glove_model(glove_file):\n    glove_model = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array([float(val) for val in split_line[1:]])\n            glove_model[word] = embedding\n    return glove_model\n\n# Load GloVe model (Provide path to your downloaded GloVe file)\nglove_file = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'  # Ensure this file is downloaded and placed in the working directory\nglove_model = load_glove_model(glove_file)\n\n# Display GloVe vector for a word (e.g., \"document\")\nprint(f\"GloVe vector for 'document': {glove_model.get('document')}\")\n\n# Calculate similarity between words (cosine similarity)\nfrom numpy.linalg import norm\n\ndef cosine_similarity(vec1, vec2):\n    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n\nword1, word2 = 'document', 'sample'\nsimilarity = cosine_similarity(glove_model.get(word1), glove_model.get(word2))\nprint(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:25:39.481004Z","iopub.execute_input":"2024-10-17T16:25:39.481676Z","iopub.status.idle":"2024-10-17T16:25:55.311563Z","shell.execute_reply.started":"2024-10-17T16:25:39.481632Z","shell.execute_reply":"2024-10-17T16:25:55.310390Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"GloVe vector for 'document': [-2.7285e-01 -9.6449e-02  4.1131e-01  3.7925e-01  8.9352e-01  4.5227e-01\n  1.9478e-01 -3.6985e-01  5.9704e-01  1.3387e-01  4.2878e-01 -2.8012e-01\n  2.0141e-01 -1.9995e-02 -6.2983e-02  7.1399e-01  8.9025e-01 -3.1009e-01\n -1.9911e-01 -4.6591e-01 -8.8145e-01 -5.4318e-01 -5.2839e-01  7.0794e-02\n -3.1042e-01 -9.8677e-01  1.0283e-01  1.6911e-01 -4.4878e-01  1.6171e-01\n  3.9394e-01  1.2655e-01 -1.2540e-01 -6.6462e-02 -1.2977e-01 -3.9406e-02\n  4.4811e-02 -4.2534e-01  2.6742e-02 -3.8609e-01 -8.4547e-01 -6.4412e-02\n  6.8974e-01  2.4521e-01 -7.3434e-01 -7.7389e-01 -1.5336e-01 -2.9057e-01\n -6.8358e-01 -3.8785e-01  1.2230e+00  1.7723e-01  1.6004e-01  8.3723e-01\n -3.1238e-01 -1.3138e+00 -2.6000e-01 -4.8754e-01  1.6751e+00  1.7320e-01\n -2.9494e-01  1.6038e-01 -5.3087e-01 -9.0950e-01  6.7436e-01 -5.2625e-01\n -3.0406e-01  8.5552e-01 -2.6879e-01 -9.0492e-01  3.0380e-01  2.0591e-01\n  3.3439e-01 -6.2308e-01  6.4306e-02  2.2179e-01 -9.2076e-02  2.1894e-01\n -1.4015e+00 -4.4588e-02  2.6263e-01  1.5343e-01 -8.8158e-04 -2.2226e-02\n -1.3228e+00 -6.3649e-02  9.7797e-01 -8.1209e-01  1.5083e-02  2.4391e-01\n -1.9343e-01 -1.7141e-01 -1.1954e-01  1.3623e-01  3.4787e-01 -5.0286e-02\n -1.8547e-01 -7.1763e-01  1.0898e-01  1.1472e-01]\nCosine similarity between 'document' and 'sample': 0.3872067855118074\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## WEEK 10: IX. QUESTION ANSWERING\n## a. BASIC Q&A SYSTEM USING KEYWORD MATCHING","metadata":{}},{"cell_type":"markdown","source":"data set was created manually \n\nfile name = week 10 .json\n\ncontent:\n\n\"root\":{5 items\n\"What is Python?\":string\"Python is a high-level programming language.\"\n\"What is Machine Learning?\":string\"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\"\n\"What is the capital of France?\":string\"The capital of France is Paris.\"\n\"Who is the president of the United States?\":string\"The current president of the United States is Joe Biden.\"\n\"How does the internet work?\":string\"The internet is a global network of computers that communicate using standardized protocols like TCP/IP.\"\n}","metadata":{}},{"cell_type":"code","source":"import json\n\n# Step 1: Load the Dataset\ndef load_dataset(file_path):\n    with open(file_path, 'r') as f:\n        return json.load(f)[\"root\"]\n\n# Step 2: Exact Matching Function with Normalization\ndef normalize_text(text):\n    # Convert text to lowercase and strip leading/trailing spaces\n    return text.lower().strip()\n\ndef find_answer(question, qa_data):\n    normalized_question = normalize_text(question)\n    \n    # Iterate through the predefined questions and check for an exact match\n    for q, a in qa_data.items():\n        normalized_q = normalize_text(q)\n        if normalized_q == normalized_question:  # Check if the question matches exactly\n            return a\n    \n    return \"Sorry, I don't know the answer to that question.\"\n\n# Step 3: Main Interaction Loop\ndef main():\n    # Load the Q&A dataset\n    qa_data = load_dataset('/kaggle/input/week10-dataset/week_10.json')\n    \n    # Start a loop for user interaction\n    while True:\n        user_question = input(\"Ask a question: \").strip()  # Get input from the user\n        if user_question.lower() in ['exit', 'quit']:  # Exit if user types 'exit' or 'quit'\n            print(\"Goodbye!\")\n            break\n        answer = find_answer(user_question, qa_data)  # Find the best answer\n        print(\"Answer:\", answer)\n\n# Step 4: Run the Q&A system\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:48:17.866198Z","iopub.execute_input":"2024-10-17T16:48:17.867172Z","iopub.status.idle":"2024-10-17T16:48:45.329266Z","shell.execute_reply.started":"2024-10-17T16:48:17.867127Z","shell.execute_reply":"2024-10-17T16:48:45.328046Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdin","text":"Ask a question:  What is Python?\n"},{"name":"stdout","text":"Answer: Python is a high-level programming language.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  what is machine learning?\n"},{"name":"stdout","text":"Answer: Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  quit\n"},{"name":"stdout","text":"Goodbye!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. BUILDING A Q&A SYSTEM WITH BERT","metadata":{}},{"cell_type":"code","source":"! pip install transformers torch tokenizers","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:49:04.862420Z","iopub.execute_input":"2024-10-17T16:49:04.862880Z","iopub.status.idle":"2024-10-17T16:49:19.242807Z","shell.execute_reply.started":"2024-10-17T16:49:04.862833Z","shell.execute_reply":"2024-10-17T16:49:19.241861Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"#: Importing the Necessary Libraries\n\nfrom transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n# Loading the Pre-trained BERT Model and Tokenizer\n\n# Load the pre-trained BERT tokenizer and model for question answering\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:49:29.980855Z","iopub.execute_input":"2024-10-17T16:49:29.981306Z","iopub.status.idle":"2024-10-17T16:49:37.168762Z","shell.execute_reply.started":"2024-10-17T16:49:29.981239Z","shell.execute_reply":"2024-10-17T16:49:37.167942Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a95cf81f3ee545dbb40599b6b547af3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35444e161d424afab2ed686be67cae1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ab57f158a0a4df08ed323966407a95f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e152c78a504e9489419d214f4bc5d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2b24a1a9c342a19601698bda5e374d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"#  Function to Answer Questions Using BERT\n\ndef answer_question(question, context):\n    # Tokenize the input question and context\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n    \n    # Get model's predicted start and end positions of the answer\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_scores = outputs.start_logits\n        end_scores = outputs.end_logits\n\n    # Get the most likely start and end token positions\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n\n    # Convert token indices to tokens and then join them into a single string answer\n    answer_tokens = inputs['input_ids'][0][start_index: end_index + 1]\n    answer = tokenizer.decode(answer_tokens)\n\n    return answer\ndef main():\n    # Get user input for context once\n    context = input(\"\\nProvide the context (paragraph): \")\n\n    while True:\n        # Get user input for the question\n        question = input(\"Ask a question (or type 'exit' to quit): \")\n\n        # Exit the system if the user types 'exit'\n        if question.lower() in ['exit', 'quit']:\n            print(\"Exiting the Q&A system.\")\n            break\n\n        # Get the answer from the BERT model\n        answer = answer_question(question, context)\n        print(f\"Answer: {answer}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:49:57.740047Z","iopub.execute_input":"2024-10-17T16:49:57.740505Z","iopub.status.idle":"2024-10-17T16:51:17.942084Z","shell.execute_reply.started":"2024-10-17T16:49:57.740460Z","shell.execute_reply":"2024-10-17T16:51:17.941213Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdin","text":"\nProvide the context (paragraph):  Machine learning (ML) is a branch of artificial intelligence (AI) that focuses on building systems capable of learning from data and making decisions or predictions without being explicitly programmed for every task. It allows computers to identify patterns in large datasets and improve their performance over time. Machine learning algorithms are designed to automatically adjust and refine their outputs based on new information, which makes them useful in tasks like image recognition, speech processing, recommendation systems, and more.\nAsk a question (or type 'exit' to quit):  what is machine Learning?\n"},{"name":"stdout","text":"Answer: a branch of artificial intelligence ( ai )\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question (or type 'exit' to quit):  quit\n"},{"name":"stdout","text":"Exiting the Q&A system.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## WEEK 11: X. MACHINE TRANSLATION\n## a. BASIC MACHINE TRANSLATION USING RULE-BASED METHODS\n","metadata":{}},{"cell_type":"code","source":"#Step 1: Define the Bilingual Dictionary\n\n# Bilingual dictionary (English to French)\ndictionary = {\n    'hello': 'bonjour',\n    'world': 'monde',\n    'my': 'mon',\n    'name': 'nom',\n    'is': 'est',\n    'good': 'bon',\n    'morning': 'matin',\n    'thank': 'merci',\n    'you': 'vous',\n    'goodbye': 'au revoir'\n}\n\n# step 2 : Define Basic Grammar Rules\n\n# Basic grammar rule: Subject-Verb-Object (SVO)\ngrammar_rules = {\n    'SVO': ['subject', 'verb', 'object']\n}\n\n#Step 3: Translation Function\n\n\ndef translate(sentence):\n    # Convert sentence to lowercase and split it into words\n    words = sentence.lower().split()\n    \n    # Translate each word using the dictionary; if the word is not in the dictionary, keep it unchanged\n    translated_words = [dictionary.get(word, word) for word in words]\n    \n    # Join the translated words back into a sentence\n    return ' '.join(translated_words)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:51:35.133631Z","iopub.execute_input":"2024-10-17T16:51:35.133974Z","iopub.status.idle":"2024-10-17T16:51:35.142350Z","shell.execute_reply.started":"2024-10-17T16:51:35.133942Z","shell.execute_reply":"2024-10-17T16:51:35.141409Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Example usage\nsentence = \"Hello world\"\nprint(translate(sentence))  # Output: bonjour monde","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:51:39.916373Z","iopub.execute_input":"2024-10-17T16:51:39.916997Z","iopub.status.idle":"2024-10-17T16:51:39.923510Z","shell.execute_reply.started":"2024-10-17T16:51:39.916960Z","shell.execute_reply":"2024-10-17T16:51:39.922576Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"bonjour monde\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence2 = \"Good morning\"\nprint(translate(sentence2))","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:51:43.116502Z","iopub.execute_input":"2024-10-17T16:51:43.116903Z","iopub.status.idle":"2024-10-17T16:51:43.125251Z","shell.execute_reply.started":"2024-10-17T16:51:43.116864Z","shell.execute_reply":"2024-10-17T16:51:43.124142Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"bon matin\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence3 = \"Thank you\"\nprint(translate(sentence3)) ","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:51:46.799942Z","iopub.execute_input":"2024-10-17T16:51:46.800865Z","iopub.status.idle":"2024-10-17T16:51:46.808104Z","shell.execute_reply.started":"2024-10-17T16:51:46.800804Z","shell.execute_reply":"2024-10-17T16:51:46.806821Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"merci vous\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. ENGLISH TO FRENCH TRANSLATION USING SEQ2SEQ WITH ATTENTION","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade tensorflow-datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-17T16:51:59.961602Z","iopub.execute_input":"2024-10-17T16:51:59.962045Z","iopub.status.idle":"2024-10-17T16:52:13.200390Z","shell.execute_reply.started":"2024-10-17T16:51:59.962004Z","shell.execute_reply":"2024-10-17T16:52:13.199449Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (4.9.6)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\nRequirement already satisfied: immutabledict in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.26.4)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.3)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (16.1.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\nRequirement already satisfied: simple-parsing in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.5)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.4.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.66.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.0)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.5.1)\nRequirement already satisfied: etils>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.7.0)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.12.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.6.1)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.0)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\nRequirement already satisfied: docstring-parser~=0.15 in /opt/conda/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets) (0.16)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\n# Step 1: Load dataset from CSV using Pandas\ndata_path = '/kaggle/input/wmt-2014-english-french/wmt14_translate_fr-en_test.csv'\ndata = pd.read_csv(data_path)\n\n# Check the first few rows and the column names of the dataframe\nprint(data.head())\nprint(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n\n# Ensure the dataframe contains the required columns\nexpected_columns = ['en', 'fr']\nassert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n\n# Step 2: Convert the DataFrame to a TensorFlow Dataset\n# Create a TensorFlow dataset from the DataFrame\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n\n# Print the first example to verify conversion\nfor english, french in train_dataset.take(1):\n    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n\n# Optional: Define constants for batch size and max length\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\n# Optional: Tokenization process\n# Tokenizer setup for input (English) and output (French)\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\ntokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n\n# Encoding function\ndef encode(en_t, fr_t):\n    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n    return en_t, fr_t\n\ndef tf_encode(en_t, fr_t):\n    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n\n# Prepare the dataset with encoding\ntrain_dataset = train_dataset.map(tf_encode)\n\n# Filter sequences longer than MAX_LENGTH\ndef filter_max_length(en, fr, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n# Print the first training example after processing\nfor en, fr in train_dataset.take(1):\n    print(f'Encoded English: {en.numpy()}')\n    print(f'Encoded French: {fr.numpy()}')\n\n# Check the shape of the batches\nfor en_batch, fr_batch in train_dataset.take(1):\n    print(f'Batch shape: English: {en_batch.shape}, French: {fr_batch.shape}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T04:07:22.532358Z","iopub.execute_input":"2024-10-18T04:07:22.533271Z","iopub.status.idle":"2024-10-18T04:07:22.805155Z","shell.execute_reply.started":"2024-10-18T04:07:22.533227Z","shell.execute_reply":"2024-10-18T04:07:22.803745Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"                                                  en  \\\n0              Spectacular Wingsuit Jump Over Bogota   \n1  Sportsman Jhonathan Florez jumped from a helic...   \n2  Wearing a wingsuit, he flew past over the famo...   \n3                           A black box in your car?   \n4  As America's road planners struggle to find th...   \n\n                                                  fr  \n0  Spectaculaire saut en \"wingsuit\" au-dessus de ...  \n1  Le sportif Jhonathan Florez a sauté jeudi d'un...  \n2  Equipé d'un wingsuit (une combinaison munie d'...  \n3               Une boîte noire dans votre voiture ?  \n4  Alors que les planificateurs du réseau routier...  \nColumns in the DataFrame: ['en', 'fr']\n","output_type":"stream"},{"name":"stderr","text":"2024-10-18 04:07:22.568661: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:2393] failed to query device memory info: CUDA_ERROR_ASSERT: device-side assert triggered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(col \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m expected_columns), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV must contain \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_columns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 2: Convert the DataFrame to a TensorFlow Dataset\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create a TensorFlow dataset from the DataFrame\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Print the first example to verify conversion\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m english, french \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:107\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m--> 107\u001b[0m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mEagerTensor(value, ctx\u001b[38;5;241m.\u001b[39mdevice_name, dtype)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:605\u001b[0m, in \u001b[0;36mContext.ensure_initialized\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetRunEagerOpAsFunction(opts, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    603\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_ContextOptionsSetJitCompileRewrite(\n\u001b[1;32m    604\u001b[0m       opts, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile_rewrite)\n\u001b[0;32m--> 605\u001b[0m   context_handle \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_NewContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    607\u001b[0m   pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_DeleteContextOptions(opts)\n","\u001b[0;31mUnknownError\u001b[0m: Failed to query available memory for GPU 0"],"ename":"UnknownError","evalue":"Failed to query available memory for GPU 0","output_type":"error"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\n# Step 1: Load dataset from CSV using Pandas\ndata_path = '/kaggle/input/en-fr-translation-dataset/en-fr.csv'\ndata = pd.read_csv(data_path)\n\n# Check the first few rows and the column names of the dataframe\nprint(data.head())\nprint(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n\n# Ensure the dataframe contains the required columns\nexpected_columns = ['en', 'fr']\nassert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n\n# Step 1.1: Handle NaN values by dropping them or filling them\ndata.dropna(subset=expected_columns, inplace=True)\n\n# Step 1.2: Ensure both columns are of string type\ndata['en'] = data['en'].astype(str)\ndata['fr'] = data['fr'].astype(str)\n\n# Step 2: Convert the DataFrame to a TensorFlow Dataset\n# Create a TensorFlow dataset from the DataFrame\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n\n# Print the first example to verify conversion\nfor english, french in train_dataset.take(1):\n    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n\n# Optional: Define constants for batch size and max length\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\n# Optional: Tokenization process\n# Tokenizer setup for input (English) and output (French)\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\ntokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n\n# Encoding function\ndef encode(en_t, fr_t):\n    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n    return en_t, fr_t\n\ndef tf_encode(en_t, fr_t):\n    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n\n# Prepare the dataset with encoding\ntrain_dataset = train_dataset.map(tf_encode)\n\n# Filter sequences longer than MAX_LENGTH\ndef filter_max_length(en, fr, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n# Print the first training example after processing\nfor en, fr in train_dataset.take(1):\n    print(f'Encoded English: {en.numpy()}')\n    print(f'Encoded French: {fr.numpy()}')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T05:11:14.074895Z","iopub.execute_input":"2024-10-18T05:11:14.075417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## WEEK 12: XI.DIALOGUE SYSTEM\n## a. BASIC RULE-BASED CHATBOT USING PYTHON NLTK","metadata":{}},{"cell_type":"code","source":"# Step 2: Import Libraries\nimport nltk\nfrom nltk.chat.util import Chat, reflections\n\n# Step 3: Define Rules (Predefined pairs)\npairs = [\n    (r\"my name is (.*)\", [\"Hello %1, How are you today?\"]),\n    (r\"hi|hey|hello\", [\"Hello\", \"Hey there\"]),\n    (r\"what is your name?\", [\"I am a bot created by [Your Name].\"]),\n    (r\"how are you?\", [\"I'm doing good. How about you?\"]),\n    (r\"sorry (.*)\", [\"No problem\", \"It's okay\", \"You don't need to be sorry\"]),\n    (r\"quit\", [\"Bye! Take care.\"])\n]\n\n# Step 4: Create the Chatbot\ndef chatbot():\n    print(\"Hi, I'm the chatbot you created. Type 'quit' to exit.\") \n    chat = Chat(pairs, reflections)\n    chat.converse()\n    \n# Step 5: Run the Chatbot\nif __name__ == \"__main__\":\n    chatbot()","metadata":{"execution":{"iopub.status.busy":"2024-10-17T17:03:04.609532Z","iopub.execute_input":"2024-10-17T17:03:04.610179Z","iopub.status.idle":"2024-10-17T17:04:27.468407Z","shell.execute_reply.started":"2024-10-17T17:03:04.610135Z","shell.execute_reply":"2024-10-17T17:04:27.467256Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Hi, I'm the chatbot you created. Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> hello, my name is fredrick\n"},{"name":"stdout","text":"Hello\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> how are you today\n"},{"name":"stdout","text":"I'm doing good. How about you?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> quit\n"},{"name":"stdout","text":"Bye! Take care.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. BUILDING A CHATBOT USING SEQ2SEQ MODELS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Step 1: Load and Preprocess the Dataset\ndef load_data(filepath):\n    try:\n        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n            lines = f.readlines()\n    except Exception as e:\n        print(f\"Error reading the file: {e}\")\n        return []\n\n    conversations = []\n    for line in lines:\n        line_parts = line.strip().split(' +++$+++ ')\n        if len(line_parts) == 5:\n            conversations.append(line_parts[4])  # Store only the dialogue part\n\n    print(f\"Loaded {len(conversations)} conversations.\")  # Debug info\n    return conversations\n\ndef create_pairs(conversations):\n    input_texts = []\n    target_texts = []\n\n    for i in range(len(conversations) - 1):\n        input_text = conversations[i]\n        target_text = conversations[i + 1]\n        target_text = '\\t' + target_text + '\\n'  # Add start and end tokens\n        input_texts.append(input_text)\n        target_texts.append(target_text)\n\n    print(f\"Created {len(input_texts)} input-target pairs.\")  # Debug info\n    return input_texts, target_texts\n\n# Load the dataset (replace with the correct path to movie_lines.txt)\nconversations = load_data('/kaggle/input/week12-dataset/movie_lines_rev 2.txt')  # Make sure this file exists\ninput_texts, target_texts = create_pairs(conversations)\n\n# Check if input_texts and target_texts are populated\nif not input_texts or not target_texts:\n    raise ValueError(\"No input or target texts were created. Please check the dataset.\")\n\n# Step 2: Tokenize and Pad the Data\n# Tokenize the input and output data\ninput_tokenizer = Tokenizer()\ntarget_tokenizer = Tokenizer()\n\ninput_tokenizer.fit_on_texts(input_texts)\ntarget_tokenizer.fit_on_texts(target_texts)\n\ninput_sequences = input_tokenizer.texts_to_sequences(input_texts)\ntarget_sequences = target_tokenizer.texts_to_sequences(target_texts)\n\n# Pad sequences to ensure uniform length\nmax_encoder_seq_length = max(len(seq) for seq in input_sequences) if input_sequences else 0\nmax_decoder_seq_length = max(len(seq) for seq in target_sequences) if target_sequences else 0\n\nencoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\ndecoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n\n# Prepare decoder output data\ndecoder_output_data = np.zeros((len(target_sequences), max_decoder_seq_length, len(target_tokenizer.word_index) + 1), dtype='float32')\n\nfor i, seq in enumerate(target_sequences):\n    for t, word_idx in enumerate(seq):\n        if t > 0:\n            decoder_output_data[i, t - 1, word_idx] = 1.0\n\n# Step 3: Build the Seq2Seq Model\nnum_encoder_tokens = len(input_tokenizer.word_index) + 1\nnum_decoder_tokens = len(target_tokenizer.word_index) + 1\n\n# Encoder\nencoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\nencoder_lstm = LSTM(256, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n\n# Save the encoder states to pass to the decoder\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Step 4: Compile and Train the Model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model (adjust epochs and batch size as needed)\nmodel.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=1)\n\n# Step 5: Inference Setup (for generating responses)\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# Step 6: Decode a Sequence (Generate a Response)\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate an empty target sequence with only the start token\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample the next token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n        decoded_sentence += sampled_char\n\n        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence and states\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n    return decoded_sentence.strip()  # Trim any extra whitespace\n\n# Step 7: Test the Chatbot\ndef chat():\n    print(\"Chatbot is ready! Type 'quit' to exit.\")\n    while True:\n        input_text = input(\"You: \")\n        if input_text.lower() == 'quit':\n            print(\"Exiting the chat. Goodbye!\")\n            break\n\n        input_sequence = input_tokenizer.texts_to_sequences([input_text])\n        input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n        response = decode_sequence(input_sequence)\n        print(f\"Bot: {response}\")\n\nif __name__ == \"__main__\":\n    chat()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T06:07:43.925247Z","iopub.execute_input":"2024-10-18T06:07:43.926015Z","iopub.status.idle":"2024-10-18T06:07:54.921115Z","shell.execute_reply.started":"2024-10-18T06:07:43.925962Z","shell.execute_reply":"2024-10-18T06:07:54.919714Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Loaded 1153 conversations.\nCreated 1152 input-target pairs.\n\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 9.1697e-04 - loss: 0.4803\nChatbot is ready! Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hellow\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 165\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 161\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m input_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([input_text])\n\u001b[1;32m    160\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_encoder_seq_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[7], line 127\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Generate an empty target sequence with only the start token\u001b[39;00m\n\u001b[1;32m    126\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 127\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    129\u001b[0m stop_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n","\u001b[0;31mKeyError\u001b[0m: '\\t'"],"ename":"KeyError","evalue":"'\\t'","output_type":"error"}]}]}