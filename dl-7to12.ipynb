{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1105996,"sourceType":"datasetVersion","datasetId":619369},{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814},{"sourceId":7685004,"sourceType":"datasetVersion","datasetId":4484220},{"sourceId":9652505,"sourceType":"datasetVersion","datasetId":5896023},{"sourceId":9656312,"sourceType":"datasetVersion","datasetId":5898896}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# WEEK 7: VI. TEXT SUMMARIZATION\n## a. BASIC TEXT SUMMARIZATION USING TF-IDF AND COSINE SIMILARITY\n","metadata":{}},{"cell_type":"code","source":"# 1. Import Required Libraries\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Download necessary datasets for tokenization and stopwords\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# 2. Define Sample Text\ntext = \"\"\"\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial\nintelligence concerned with the interactions between computers and human language, in\nparticular how to program computers to process and analyze large amounts of natural language\ndata.\nChallenges in natural language processing frequently involve speech recognition, natural\nlanguage understanding, and natural language generation.\n\"\"\"\n\n# 3. Preprocess the Text\n# Split the text into sentences\nsentences = nltk.sent_tokenize(text)\n\n# Get the set of stopwords in English\nstop_words = set(stopwords.words('english'))\n\n# Function to preprocess each sentence by removing stopwords\ndef preprocess_sentence(sentence):\n    return ' '.join([word for word in sentence.split() if word.lower() not in stop_words])\n\n# Preprocess all the sentences\npreprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n\n# 4. Compute TF-IDF Matrix\n# Create a TfidfVectorizer object\nvectorizer = TfidfVectorizer()\n\n# Transform the preprocessed sentences into TF-IDF features\ntfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n\n# 5. Compute Cosine Similarity\n# Compute cosine similarity between TF-IDF vectors of the sentences\ncosine_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\n# 6. Generate Summary\n# Function to generate a summary by ranking sentences based on their similarity scores\ndef generate_summary(sentences, sim_matrix, top_n=2):\n    # Compute the sum of similarity scores for each sentence\n    scores = sim_matrix.sum(axis=1)\n    \n    # Rank sentences based on the scores and select the top 'n' sentences\n    ranked_sentences = [sentences[i] for i in scores.argsort()[-top_n:]]\n    \n    # Return the summary as a string\n    return ' '.join(ranked_sentences)\n\n# Generate and print the summary\nsummary = generate_summary(sentences, cosine_sim_matrix)\nprint(\"Summary:\")\nprint(summary)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T07:01:19.904985Z","iopub.execute_input":"2024-10-18T07:01:19.905352Z","iopub.status.idle":"2024-10-18T07:01:21.939204Z","shell.execute_reply.started":"2024-10-18T07:01:19.905308Z","shell.execute_reply":"2024-10-18T07:01:21.938050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nSummary:\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial\nintelligence concerned with the interactions between computers and human language, in\nparticular how to program computers to process and analyze large amounts of natural language\ndata. Challenges in natural language processing frequently involve speech recognition, natural\nlanguage understanding, and natural language generation.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. ABSTRACTIVE TEXT SUMMARIZATION WITH TRANSFORMERS","metadata":{}},{"cell_type":"code","source":"! pip install transformers datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:01:30.499604Z","iopub.execute_input":"2024-10-18T07:01:30.500053Z","iopub.status.idle":"2024-10-18T07:01:44.180252Z","shell.execute_reply.started":"2024-10-18T07:01:30.500017Z","shell.execute_reply":"2024-10-18T07:01:44.179013Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# complte code 7 week 2 question\n\n# 1. Install required libraries (run this in your environment first)\n# !pip install transformers datasets\n\n# 2. Import Required Libraries\nfrom transformers import BartForConditionalGeneration, BartTokenizer\nfrom datasets import load_dataset\n\n# 3. Load the Dataset\n# Load the CNN/DailyMail dataset (test split, 1% for demonstration purposes)\ndataset = load_dataset('cnn_dailymail', '3.0.0', split='test[:1%]')\n\n# 4. Load Pre-trained BART Model and Tokenizer\n# Load pre-trained BART model and tokenizer\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n\n# 5. Summarize Text\n# Function to summarize text\ndef summarize(text):\n    # Tokenize the input text\n    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    \n    # Generate the summary\n    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n    \n    # Decode the generated summary\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\n# Sample Input and Output\n# Summarize a few sample articles from the dataset\nfor i in range(3):  # Loop through first 3 samples for demonstration\n    article = dataset[i]['article']\n    print(f\"Original Text {i+1}: {article}\\n\")\n    \n    # Generate and print the summary\n    summary = summarize(article)\n    print(f\"Summary {i+1}: {summary}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:01:55.013764Z","iopub.execute_input":"2024-10-18T07:01:55.014230Z","iopub.status.idle":"2024-10-18T07:02:50.564196Z","shell.execute_reply.started":"2024-10-18T07:01:55.014181Z","shell.execute_reply":"2024-10-18T07:02:50.563142Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ee2dfac18546d28f5ba45d608cf532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b86dfd094d6450e97434fb273a0da78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"263dadff2cde49cf829c0ec21c226a18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e949b7ca2e41ff8e8e72e6dc5c88d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa51cbb385124301b5e648bd44d72c0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebdef85fa34b4895b4523908edbbf6d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1266eab39aae4f53abd24e44dff701d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36bcf5b3702a42619c710fafded0a733"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a855c0fb59647e3b9ac49f9f996fcd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd0d159fb6774002ab5f102cd40f0148"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ad08d84dd2742938c0438957a90a3c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cca00a50c732463eb5590192bd68c103"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4658de5f3054c5c909a19e4549a76af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa96da10dc3d459aa734ec400b45fe79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5f69ebb32774bf5ae17946a932acd6c"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Original Text 1: (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should immediately end their pressure, and countries that support universal acceptance of the court's treaty should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the group. \"What's objectionable is the attempts to undermine international justice, not Palestine's decision to join a treaty to which over 100 countries around the world are members.\" In January, when the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court's decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder and Faith Karimi contributed to this report.\n\nSummary 1: The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the Palestinians' efforts to join the body.\n\nOriginal Text 2: (CNN)Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hammer in a misguided mercy killing and then buried in a field -- only to survive. That's according to Washington State University, where the dog -- a friendly white-and-black bully breed mix now named Theia -- has been receiving care at the Veterinary Teaching Hospital. Four days after her apparent death, the dog managed to stagger to a nearby farm, dirt-covered and emaciated, where she was found by a worker who took her to a vet for help. She was taken in by Moses Lake, Washington, resident Sara Mellado. \"Considering everything that she's been through, she's incredibly gentle and loving,\" Mellado said, according to WSU News. \"She's a true miracle dog and she deserves a good life.\" Theia is only one year old but the dog's brush with death did not leave her unscathed. She suffered a dislocated jaw, leg injuries and a caved-in sinus cavity -- and still requires surgery to help her breathe. The veterinary hospital's Good Samaritan Fund committee awarded some money to help pay for the dog's treatment, but Mellado has set up a fundraising page to help meet the remaining cost of the dog's care. She's also created a Facebook page to keep supporters updated. Donors have already surpassed the $10,000 target, inspired by Theia's tale of survival against the odds. On the fundraising page, Mellado writes, \"She is in desperate need of extensive medical procedures to fix her nasal damage and reset her jaw. I agreed to foster her until she finally found a loving home.\" She is dedicated to making sure Theia gets the medical attention she needs, Mellado adds, and wants to \"make sure she gets placed in a family where this will never happen to her again!\" Any additional funds raised will be \"paid forward\" to help other animals. Theia is not the only animal to apparently rise from the grave in recent weeks. A cat in Tampa, Florida, found seemingly dead after he was hit by a car in January, showed up alive in a neighbor's yard five days after he was buried by his owner. The cat was in bad shape, with maggots covering open wounds on his body and a ruined left eye, but remarkably survived with the help of treatment from the Humane Society.\n\nSummary 2: Theia, a one-year-old bully breed mix, was hit by a car and buried in a field. She managed to stagger to a nearby farm, dirt-covered and emaciated. She suffered a dislocated jaw, leg injuries and a caved-in sinus cavity -- and still requires surgery to help her breathe.\n\nOriginal Text 3: (CNN)If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U.S. Secretary of State John Kerry's opposite number in securing a breakthrough in nuclear discussions that could lead to an end to sanctions against Iran -- if the details can be worked out in the coming weeks. And he received a hero's welcome as he arrived in Iran on a sunny Friday morning. \"Long live Zarif,\" crowds chanted as his car rolled slowly down the packed street. You may well have read that he is \"polished\" and, unusually for one burdened with such weighty issues, \"jovial.\" An Internet search for \"Mohammad Javad Zarif\" and \"jovial\" yields thousands of results. He certainly has gone a long way to bring Iran in from the cold and allow it to rejoin the international community. But there are some facts about Zarif that are less well-known. Here are six: . In September 2013, Zarif tweeted \"Happy Rosh Hashanah,\" referring to the Jewish New Year. That prompted Christine Pelosi, the daughter of House Minority Leader Nancy Pelosi, to respond with a tweet of her own: \"Thanks. The New Year would be even sweeter if you would end Iran's Holocaust denial, sir.\" And, perhaps to her surprise, Pelosi got a response. \"Iran never denied it,\" Zarif tweeted back. \"The man who was perceived to be denying it is now gone. Happy New Year.\" The reference was likely to former Iranian President Mahmoud Ahmadinejad, who had left office the previous month. Zarif was nominated to be foreign minister by Ahmadinejad's successor, Hassan Rouhami. His foreign ministry notes, perhaps defensively, that \"due to the political and security conditions of the time, he decided to continue his education in the United States.\" That is another way of saying that he was outside the country during the demonstrations against the Shah of Iran, which began in 1977, and during the Iranian Revolution, which drove the shah from power in 1979. Zarif left the country in 1977, received his undergraduate degree from San Francisco State University in 1981, his master's in international relations from the University of Denver in 1984 and his doctorate from the University of Denver in 1988. Both of his children were born in the United States. The website of the Iranian Foreign Ministry, which Zarif runs, cannot even agree with itself on when he was born. The first sentence of his official biography, perhaps in a nod to the powers that be in Tehran, says Zarif was \"born to a religious traditional family in Tehran in 1959.\" Later on the same page, however, his date of birth is listed as January 8, 1960. And the Iranian Diplomacy website says he was born in in 1961 . So he is 54, 55 or maybe even 56. Whichever, he is still considerably younger than his opposite number, Kerry, who is 71. The feds investigated him over his alleged role in controlling the Alavi Foundation, a charitable organization. The U.S. Justice Department said the organization was secretly run on behalf of the Iranian government to launder money and get around U.S. sanctions. But last year, a settlement in the case, under which the foundation agreed to give a 36-story building in Manhattan along with other properties to the U.S. government, did not mention Zarif's name. Early in the Iranian Revolution, Zarif was among the students who took over the Iranian Consulate in San Francisco. The aim, says the website Iranian.com -- which cites Zarif's memoirs, titled \"Mr. Ambassador\" -- was to expel from the consulate people who were not sufficiently Islamic. Later, the website says, Zarif went to make a similar protest at the Iranian mission to the United Nations. In response, the Iranian ambassador to the United Nations offered him a job. In fact, he has now spent more time with Kerry than any other foreign minister in the world. And that amount of quality time will only increase as the two men, with help from other foreign ministers as well, try to meet a June 30 deadline for nailing down the details of the agreement they managed to outline this week in Switzerland.\n\nSummary 3: Mohammad Javad Zarif is the Iranian foreign minister. He has been John Kerry's opposite number in securing a breakthrough in nuclear discussions. But there are some facts about Zarif that are less well-known.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# WEEK 8: VII. TEXT ENTAILMENT APPLICATIONS IN PYTHON\n## a. BASIC TEXT ENTAILMENT USING SIMPLE RULE-BASED METHODS","metadata":{}},{"cell_type":"code","source":"# 1. Import necessary libraries and load dataset\nfrom datasets import load_dataset\nimport pandas as pd\nimport nltk\nfrom sklearn.metrics import accuracy_score\n\n# Download NLTK tokenizers\nnltk.download('punkt')\n\n# Sample dataset (as CNN/DailyMail isn't well-suited for entailment tasks, we'll create sample pairs)\ndata = pd.DataFrame({\n    'sentence1': [\"The cat is on the mat.\", \"The sun is shining brightly.\", \"The game is over.\"],\n    'sentence2': [\"The mat has a cat.\", \"The sky is bright.\", \"The players are done playing.\"],\n    'label': [False, True, True]  # Labels for entailment (True/False)\n})\n\n# 2. Preprocess the data: tokenize and convert to lowercase\ndef preprocess(text):\n    return nltk.word_tokenize(text.lower())\n\n# Apply preprocessing to both sentences\ndata['sentence1_tokens'] = data['sentence1'].apply(preprocess)\ndata['sentence2_tokens'] = data['sentence2'].apply(preprocess)\n\n# 3. Define simple rule-based method for text entailment\ndef simple_rule_based_entailment(s1, s2):\n    return set(s2).issubset(set(s1))\n\n# Apply the rule-based entailment check\ndata['prediction'] = data.apply(lambda row: simple_rule_based_entailment(row['sentence1_tokens'], row['sentence2_tokens']), axis=1)\n\n# 4. Evaluate the model\naccuracy = accuracy_score(data['label'], data['prediction'])\nprint(f'Accuracy: {accuracy}')\n\n# Output the data for reference\nprint(data[['sentence1', 'sentence2', 'label', 'prediction']])","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:02:59.823068Z","iopub.execute_input":"2024-10-18T07:02:59.823804Z","iopub.status.idle":"2024-10-18T07:02:59.858525Z","shell.execute_reply.started":"2024-10-18T07:02:59.823764Z","shell.execute_reply":"2024-10-18T07:02:59.857456Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nAccuracy: 0.3333333333333333\n                      sentence1                      sentence2  label  \\\n0        The cat is on the mat.             The mat has a cat.  False   \n1  The sun is shining brightly.             The sky is bright.   True   \n2             The game is over.  The players are done playing.   True   \n\n   prediction  \n0       False  \n1       False  \n2       False  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b.NATURAL LANGUAGE INFERENCE WITH BERT","metadata":{}},{"cell_type":"code","source":"# Step 1: Import Required Libraries\nfrom datasets import load_dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\n\n# Step 2: Load the Dataset\ndataset = load_dataset('snli')\n\n# Check the first few examples to understand the structure\nprint(dataset['train'].features)  # Check the features of the training dataset\nprint(dataset['train'][0:5])       # Print the first 5 examples from the training dataset\n\n# Step 3: Preprocess the Data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef preprocess_function(examples):\n    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length', max_length=128)\n\n# Apply preprocessing to the dataset (train, validation, and test splits)\nencoded_dataset = dataset.map(preprocess_function, batched=True)\n\n# Check the structure of the dataset again\nprint(encoded_dataset)\n\n# Step 4: Inspect the label column directly to understand its structure\nprint(\"Label examples:\")\nprint(encoded_dataset['train']['label'][0:5])  # Print the first 5 labels\n\n# Step 5: Identify unique labels\nunique_labels = set(encoded_dataset['train']['label'])\nprint(f\"Unique labels in the dataset: {unique_labels}\")\n\n# Step 6: Define label mapping and handle unexpected labels\nlabel_dict = {0: 0, 1: 1, 2: 2}  # Adjust this as necessary based on your labels\n\n# Step 7: Map the labels correctly, handle unexpected labels\ndef map_labels(example):\n    # Use the label_dict for mapping, and set a default for unexpected labels\n    label = example['label']\n    return {'labels': label_dict.get(label, -1)}  # Map to -1 if the label is unexpected\n\nencoded_dataset = encoded_dataset.map(map_labels)\n\n# Set the format for PyTorch\nencoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n\n# Step 8: Load the Pre-Trained BERT Model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n\n# Step 9: Set Up Training Arguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir='./results',          # Output directory\n    evaluation_strategy='epoch',     # Evaluation during each epoch\n    per_device_train_batch_size=16,  # Batch size for training\n    per_device_eval_batch_size=16,   # Batch size for evaluation\n    num_train_epochs=3,              # Number of training epochs\n    weight_decay=0.01,               # Strength of L2 regularization\n    logging_dir='./logs',            # Directory for logs\n)\n\n# Initialize the Trainer with the model, training arguments, and datasets\ntrainer = Trainer(\n    model=model,                         # The BERT model for training\n    args=training_args,                  # Training arguments\n    train_dataset=encoded_dataset['train'],  # Training dataset\n    eval_dataset=encoded_dataset['validation'],  # Validation dataset\n)\n\n# Step 10: Train the Model\ntrainer.train()\n\n# Step 11: Evaluate the Model\neval_results = trainer.evaluate()\nprint(f\"Evaluation Results: {eval_results}\")\n\n# Step 12: Make Predictions\npremise = \"A man inspects the uniform of a figure in some East Asian country.\"\nhypothesis = \"The man is sleeping.\"\n\n# Tokenize the input example\ninputs = tokenizer(premise, hypothesis, return_tensors='pt', padding=True, truncation=True, max_length=128)\n\n# Get model prediction\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predicted_label = torch.argmax(outputs.logits).item()\n\n# Convert prediction to human-readable label\nlabel_map = {0: 'entailment', 1: 'contradiction', 2: 'neutral'}\nprint(f\"Predicted Label: {label_map[predicted_label]}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:03:04.394631Z","iopub.execute_input":"2024-10-18T07:03:04.394993Z","iopub.status.idle":"2024-10-18T07:14:15.969834Z","shell.execute_reply.started":"2024-10-18T07:03:04.394960Z","shell.execute_reply":"2024-10-18T07:14:15.968460Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44e572aa74cf40e19ca7e20162b43dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/412k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c0f60036f90499183041e16625a46b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/413k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ebf4e4447c4e8084980011d4d707d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/19.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8ef6a745a984094b18a6f2f2046ff8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12fa04fb430849f3a1019516358f6b28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d21bd885f64ed7aad744ead537cf31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/550152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46010e22232e47c2a54c725919db80cf"}},"metadata":{}},{"name":"stdout","text":"{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}\n{'premise': ['A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.', 'Children smiling and waving at camera', 'Children smiling and waving at camera'], 'hypothesis': ['A person is training his horse for a competition.', 'A person is at a diner, ordering an omelette.', 'A person is outdoors, on a horse.', 'They are smiling at their parents', 'There are children present'], 'label': [1, 2, 0, 1, 0]}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7594b1051a1a4ff78492374999f00ed9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3582cc33b2dd4392affdf91fcd7adf5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046116074ea046ab97c2f646eabca95b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57519aebd7704fb18415c82dbd9512ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd4dbaa8b4746b5b9bf0afa58798a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef4c43bf4074d1db7f87343be892f32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/550152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac3f51cbb4b486ca358026666a59b30"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    test: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 10000\n    })\n    validation: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['premise', 'hypothesis', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 550152\n    })\n})\nLabel examples:\n[1, 2, 0, 1, 0]\nUnique labels in the dataset: {0, 1, 2, -1}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7828183eb7c4297b3f628fc5feda2d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ea953ac7ef43b7b19e9ed252d85182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/550152 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e987f6c1eb24fbeb77e80ac2ec79af6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6618bb10f7d4ebebfa547ba265c5d9c"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111341198889022, max=1.0)â€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681c3d79bbc74a2c8471eeeada9cb68d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241018_071409-zn7g7avz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ds235229106-bishop-heber-college/huggingface/runs/zn7g7avz' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/ds235229106-bishop-heber-college/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ds235229106-bishop-heber-college/huggingface' target=\"_blank\">https://wandb.ai/ds235229106-bishop-heber-college/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ds235229106-bishop-heber-college/huggingface/runs/zn7g7avz' target=\"_blank\">https://wandb.ai/ds235229106-bishop-heber-college/huggingface/runs/zn7g7avz</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8' max='103155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [     8/103155 00:01 < 5:41:41, 5.03 it/s, Epoch 0.00/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 70\u001b[0m\n\u001b[1;32m     62\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     63\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                         \u001b[38;5;66;03m# The BERT model for training\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# Training arguments\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Training dataset\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Validation dataset\u001b[39;00m\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Step 10: Train the Model\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Step 11: Evaluate the Model\u001b[39;00m\n\u001b[1;32m     73\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2390\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"markdown","source":"## WEEK 9: VIII. WORD AND SENTENCE EMBEDDING \n## a. BASIC WORD EMBEDDINGS WITH TF-IDF","metadata":{}},{"cell_type":"code","source":"#a\n# Step 1: Import Required Libraries\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Step 2: Load the Dataset\nnewsgroups = fetch_20newsgroups(subset='train')\ntexts = newsgroups.data  # Extract the document texts\n\n# Step 3: Preprocess the Text Data\nvectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\nX_tfidf = vectorizer.fit_transform(texts)  # Fit and transform the text data\n\n# Step 4: Explore the TF-IDF Matrix\nprint(\"TF-IDF matrix shape:\", X_tfidf.shape)  # Display shape of the matrix\nX_dense = X_tfidf.todense()  # Convert to dense format for better visualization\nprint(X_dense[0])  # Print the first document's TF-IDF vector\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:14:21.797552Z","iopub.execute_input":"2024-10-18T07:14:21.798197Z","iopub.status.idle":"2024-10-18T07:14:35.666444Z","shell.execute_reply.started":"2024-10-18T07:14:21.798155Z","shell.execute_reply":"2024-10-18T07:14:35.665241Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"TF-IDF matrix shape: (11314, 1000)\n[[0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.12190754 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.16779786 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.13299938 0.         0.         0.         0.73410701\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.15018239 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.12747266 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.16271777 0.         0.\n  0.         0.         0.         0.10513674 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.18701637 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.16060438 0.         0.         0.         0.\n  0.         0.06818803 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.14078947 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.08240921 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.03699896 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.17395851\n  0.13299938 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.1153094\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.17026123 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.06867111\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.0383646  0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.06636009 0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.10908571 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.15730232\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.16291604 0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.14488165 0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.03687817\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.10609305 0.         0.11962569 0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.07139339 0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.         0.         0.         0.         0.         0.\n  0.11670224 0.         0.         0.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. GENERATING WORD EMBEDDINGS USING WORD2VEC AND GLOVE","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nimport re\n\n# Load a sample corpus (For demonstration, we'll use some random sentences)\ncorpus = [\n    \"This is a sample document.\",\n    \"Another example document.\",\n    \"Word embeddings capture semantic relationships.\",\n    \"GloVe and Word2Vec are popular embedding methods.\"\n]\n\n# Preprocess the text data (Tokenize and remove stop words)\nstop_words = stopwords.words('english')\n\ndef preprocess(text):\n    # Remove special characters, convert to lowercase, and tokenize\n    return [word for word in simple_preprocess(text) if word not in stop_words]\n\n# Apply preprocessing to the corpus\ntokenized_corpus = [preprocess(doc) for doc in corpus]\n\n# Display tokenized text\nprint(tokenized_corpus)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:14:41.908810Z","iopub.execute_input":"2024-10-18T07:14:41.909169Z","iopub.status.idle":"2024-10-18T07:15:10.372650Z","shell.execute_reply.started":"2024-10-18T07:14:41.909136Z","shell.execute_reply":"2024-10-18T07:15:10.371648Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[['sample', 'document'], ['another', 'example', 'document'], ['word', 'embeddings', 'capture', 'semantic', 'relationships'], ['glove', 'word', 'vec', 'popular', 'embedding', 'methods']]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train word2vex model\n\n# Import necessary library\nfrom gensim.models import Word2Vec\n\n# Train Word2Vec model (Skip-gram model, vector_size=100, window=5)\nword2vec_model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, sg=1, min_count=1)\n\n# Display the vector for a sample word (e.g., \"document\")\nword_vector = word2vec_model.wv['document']\nprint(f\"Word2Vec vector for 'document': {word_vector}\")\n\n# Find similar words to 'document'\nsimilar_words = word2vec_model.wv.most_similar('document')\nprint(f\"Words similar to 'document': {similar_words}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T15:37:16.073744Z","iopub.execute_input":"2024-10-06T15:37:16.074836Z","iopub.status.idle":"2024-10-06T15:37:16.093825Z","shell.execute_reply.started":"2024-10-06T15:37:16.074791Z","shell.execute_reply":"2024-10-06T15:37:16.092668Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Word2Vec vector for 'document': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\nWords similar to 'document': [('semantic', 0.21617142856121063), ('another', 0.09310110658407211), ('glove', 0.09291722625494003), ('example', 0.07963486760854721), ('embeddings', 0.06285078823566437), ('capture', 0.0270574688911438), ('relationships', 0.016134677454829216), ('word', -0.010839167051017284), ('methods', -0.027750369161367416), ('sample', -0.04125341773033142)]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\n\n# Load pre-trained GloVe vectors (assuming 'glove.6B.100d.txt' is downloaded and available)\ndef load_glove_model(glove_file):\n    glove_model = {}\n    with open(glove_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            split_line = line.split()\n            word = split_line[0]\n            embedding = np.array([float(val) for val in split_line[1:]])\n            glove_model[word] = embedding\n    return glove_model\n\n# Load GloVe model (Provide path to your downloaded GloVe file)\nglove_file = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'  # Ensure this file is downloaded and placed in the working directory\nglove_model = load_glove_model(glove_file)\n\n# Display GloVe vector for a word (e.g., \"document\")\nprint(f\"GloVe vector for 'document': {glove_model.get('document')}\")\n\n# Calculate similarity between words (cosine similarity)\nfrom numpy.linalg import norm\n\ndef cosine_similarity(vec1, vec2):\n    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n\nword1, word2 = 'document', 'sample'\nsimilarity = cosine_similarity(glove_model.get(word1), glove_model.get(word2))\nprint(f\"Cosine similarity between '{word1}' and '{word2}': {similarity}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:15:25.124516Z","iopub.execute_input":"2024-10-18T07:15:25.124951Z","iopub.status.idle":"2024-10-18T07:15:39.846613Z","shell.execute_reply.started":"2024-10-18T07:15:25.124912Z","shell.execute_reply":"2024-10-18T07:15:39.845459Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"GloVe vector for 'document': [-2.7285e-01 -9.6449e-02  4.1131e-01  3.7925e-01  8.9352e-01  4.5227e-01\n  1.9478e-01 -3.6985e-01  5.9704e-01  1.3387e-01  4.2878e-01 -2.8012e-01\n  2.0141e-01 -1.9995e-02 -6.2983e-02  7.1399e-01  8.9025e-01 -3.1009e-01\n -1.9911e-01 -4.6591e-01 -8.8145e-01 -5.4318e-01 -5.2839e-01  7.0794e-02\n -3.1042e-01 -9.8677e-01  1.0283e-01  1.6911e-01 -4.4878e-01  1.6171e-01\n  3.9394e-01  1.2655e-01 -1.2540e-01 -6.6462e-02 -1.2977e-01 -3.9406e-02\n  4.4811e-02 -4.2534e-01  2.6742e-02 -3.8609e-01 -8.4547e-01 -6.4412e-02\n  6.8974e-01  2.4521e-01 -7.3434e-01 -7.7389e-01 -1.5336e-01 -2.9057e-01\n -6.8358e-01 -3.8785e-01  1.2230e+00  1.7723e-01  1.6004e-01  8.3723e-01\n -3.1238e-01 -1.3138e+00 -2.6000e-01 -4.8754e-01  1.6751e+00  1.7320e-01\n -2.9494e-01  1.6038e-01 -5.3087e-01 -9.0950e-01  6.7436e-01 -5.2625e-01\n -3.0406e-01  8.5552e-01 -2.6879e-01 -9.0492e-01  3.0380e-01  2.0591e-01\n  3.3439e-01 -6.2308e-01  6.4306e-02  2.2179e-01 -9.2076e-02  2.1894e-01\n -1.4015e+00 -4.4588e-02  2.6263e-01  1.5343e-01 -8.8158e-04 -2.2226e-02\n -1.3228e+00 -6.3649e-02  9.7797e-01 -8.1209e-01  1.5083e-02  2.4391e-01\n -1.9343e-01 -1.7141e-01 -1.1954e-01  1.3623e-01  3.4787e-01 -5.0286e-02\n -1.8547e-01 -7.1763e-01  1.0898e-01  1.1472e-01]\nCosine similarity between 'document' and 'sample': 0.3872067855118074\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## WEEK 10: IX. QUESTION ANSWERING\n## a. BASIC Q&A SYSTEM USING KEYWORD MATCHING","metadata":{}},{"cell_type":"markdown","source":"data set was created manually \n\nfilte name = week 10 .json\nWhat is Python\ncontent:\n\n\"root\":{5 items\n\"What is Python?\":string\"Python is a high-level programming language.\"\n\"What is Machine Learning?\":string\"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\"\n\"What is the capital of France?\":string\"The capital of France is Paris.\"\n\"Who is the president of the United States?\":string\"The current president of the United States is Joe Biden.\"\n\"How does the internet work?\":string\"The internet is a global network of computers that communicate using standardized protocols like TCP/IP.\"\n}","metadata":{}},{"cell_type":"code","source":"import json\n\n# Step 1: Load the Dataset\ndef load_dataset(file_path):\n    with open(file_path, 'r') as f:\n        return json.load(f)[\"root\"]\n\n# Step 2: Exact Matching Function with Normalization\ndef normalize_text(text):\n    # Convert text to lowercase and strip leading/trailing spaces\n    return text.lower().strip()\n\ndef find_answer(question, qa_data):\n    normalized_question = normalize_text(question)\n    \n    # Iterate through the predefined questions and check for an exact match\n    for q, a in qa_data.items():\n        normalized_q = normalize_text(q)\n        if normalized_q == normalized_question:  # Check if the question matches exactly\n            return a\n    \n    return \"Sorry, I don't know the answer to that question.\"\n\n# Step 3: Main Interaction Loop\ndef main():\n    # Load the Q&A dataset\n    qa_data = load_dataset('/kaggle/input/week10-dataset/week_10.json')\n    \n    # Start a loop for user interaction\n    while True:\n        user_question = input(\"Ask a question: \").strip()  # Get input from the user\n        if user_question.lower() in ['exit', 'quit']:  # Exit if user types 'exit' or 'quit'\n            print(\"Goodbye!\")\n            break\n        answer = find_answer(user_question, qa_data)  # Find the best answer\n        print(\"Answer:\", answer)\n\n# Step 4: Run the Q&A system\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:20:30.998735Z","iopub.execute_input":"2024-10-18T07:20:30.999431Z","iopub.status.idle":"2024-10-18T07:21:00.476833Z","shell.execute_reply.started":"2024-10-18T07:20:30.999388Z","shell.execute_reply":"2024-10-18T07:21:00.475908Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"Ask a question:  What is Python?\n"},{"name":"stdout","text":"Answer: Python is a high-level programming language.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  What is Machine Learning?\n"},{"name":"stdout","text":"Answer: Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question:  quit\n"},{"name":"stdout","text":"Goodbye!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. BUILDING A Q&A SYSTEM WITH BERT","metadata":{}},{"cell_type":"code","source":"! pip install transformers torch tokenizers","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:21:07.479855Z","iopub.execute_input":"2024-10-18T07:21:07.481009Z","iopub.status.idle":"2024-10-18T07:21:19.425089Z","shell.execute_reply.started":"2024-10-18T07:21:07.480954Z","shell.execute_reply":"2024-10-18T07:21:19.423822Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.10/site-packages (0.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"#: Importing the Necessary Libraries\n\nfrom transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n# Loading the Pre-trained BERT Model and Tokenizer\n\n# Load the pre-trained BERT tokenizer and model for question answering\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:21:34.421143Z","iopub.execute_input":"2024-10-18T07:21:34.422088Z","iopub.status.idle":"2024-10-18T07:21:34.822833Z","shell.execute_reply.started":"2024-10-18T07:21:34.422022Z","shell.execute_reply":"2024-10-18T07:21:34.822025Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"#  Function to Answer Questions Using BERT\n\ndef answer_question(question, context):\n    # Tokenize the input question and context\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n    \n    # Get model's predicted start and end positions of the answer\n    with torch.no_grad():\n        outputs = model(**inputs)\n        start_scores = outputs.start_logits\n        end_scores = outputs.end_logits\n\n    # Get the most likely start and end token positions\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores)\n\n    # Convert token indices to tokens and then join them into a single string answer\n    answer_tokens = inputs['input_ids'][0][start_index: end_index + 1]\n    answer = tokenizer.decode(answer_tokens)\n\n    return answer\ndef main():\n    # Get user input for context once\n    context = input(\"\\nProvide the context (paragraph): \")\n\n    while True:\n        # Get user input for the question\n        question = input(\"Ask a question (or type 'exit' to quit): \")\n\n        # Exit the system if the user types 'exit'\n        if question.lower() in ['exit', 'quit']:\n            print(\"Exiting the Q&A system.\")\n            break\n\n        # Get the answer from the BERT model\n        answer = answer_question(question, context)\n        print(f\"Answer: {answer}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:21:38.404650Z","iopub.execute_input":"2024-10-18T07:21:38.405780Z","iopub.status.idle":"2024-10-18T07:22:30.172898Z","shell.execute_reply.started":"2024-10-18T07:21:38.405717Z","shell.execute_reply":"2024-10-18T07:22:30.171824Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdin","text":"\nProvide the context (paragraph):  Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance\nAsk a question (or type 'exit' to quit):  what is machine learning\n"},{"name":"stdout","text":"Answer: a field of study in artificial intelligence\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Ask a question (or type 'exit' to quit):  quit\n"},{"name":"stdout","text":"Exiting the Q&A system.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## WEEK 11: X. MACHINE TRANSLATION\n## a. BASIC MACHINE TRANSLATION USING RULE-BASED METHODS\n","metadata":{}},{"cell_type":"code","source":"#Step 1: Define the Bilingual Dictionary\n\n# Bilingual dictionary (English to French)\ndictionary = {\n    'hello': 'bonjour',\n    'world': 'monde',\n    'my': 'mon',\n    'name': 'nom',\n    'is': 'est',\n    'good': 'bon',\n    'morning': 'matin',\n    'thank': 'merci',\n    'you': 'vous',\n    'goodbye': 'au revoir'\n}\n\n# step 2 : Define Basic Grammar Rules\n\n# Basic grammar rule: Subject-Verb-Object (SVO)\ngrammar_rules = {\n    'SVO': ['subject', 'verb', 'object']\n}\n\n#Step 3: Translation Function\n\n\ndef translate(sentence):\n    # Convert sentence to lowercase and split it into words\n    words = sentence.lower().split()\n    \n    # Translate each word using the dictionary; if the word is not in the dictionary, keep it unchanged\n    translated_words = [dictionary.get(word, word) for word in words]\n    \n    # Join the translated words back into a sentence\n    return ' '.join(translated_words)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:22:34.180803Z","iopub.execute_input":"2024-10-18T07:22:34.181607Z","iopub.status.idle":"2024-10-18T07:22:34.188975Z","shell.execute_reply.started":"2024-10-18T07:22:34.181566Z","shell.execute_reply":"2024-10-18T07:22:34.187970Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Example usage\nsentence = \"Hello world\"\nprint(translate(sentence))  # Output: bonjour monde","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:22:40.006840Z","iopub.execute_input":"2024-10-18T07:22:40.007187Z","iopub.status.idle":"2024-10-18T07:22:40.014659Z","shell.execute_reply.started":"2024-10-18T07:22:40.007157Z","shell.execute_reply":"2024-10-18T07:22:40.013542Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"bonjour monde\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence2 = \"Good morning\"\nprint(translate(sentence2))","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:22:42.897968Z","iopub.execute_input":"2024-10-18T07:22:42.898964Z","iopub.status.idle":"2024-10-18T07:22:42.905563Z","shell.execute_reply.started":"2024-10-18T07:22:42.898918Z","shell.execute_reply":"2024-10-18T07:22:42.904415Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"bon matin\n","output_type":"stream"}]},{"cell_type":"code","source":"sentence3 = \"Thank you\"\nprint(translate(sentence3)) ","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:22:45.976057Z","iopub.execute_input":"2024-10-18T07:22:45.976479Z","iopub.status.idle":"2024-10-18T07:22:45.983517Z","shell.execute_reply.started":"2024-10-18T07:22:45.976441Z","shell.execute_reply":"2024-10-18T07:22:45.982643Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"merci vous\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. ENGLISH TO FRENCH TRANSLATION USING SEQ2SEQ WITH ATTENTION","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade tensorflow-datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:22:48.246033Z","iopub.execute_input":"2024-10-18T07:22:48.246812Z","iopub.status.idle":"2024-10-18T07:23:00.424231Z","shell.execute_reply.started":"2024-10-18T07:22:48.246761Z","shell.execute_reply":"2024-10-18T07:23:00.423108Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.10/site-packages (4.9.6)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\nRequirement already satisfied: immutabledict in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.26.4)\nRequirement already satisfied: promise in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.3)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (16.1.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\nRequirement already satisfied: simple-parsing in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.5)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.14.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (2.4.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (4.66.4)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.0)\nRequirement already satisfied: array-record>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-datasets) (0.5.1)\nRequirement already satisfied: etils>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.7.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.6.1)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.0)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.12.2)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\nRequirement already satisfied: docstring-parser~=0.15 in /opt/conda/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets) (0.16)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets) (1.63.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\n# Step 1: Load dataset from CSV using Pandas\ndata_path = '/kaggle/input/wmt-2014-english-french/wmt14_translate_fr-en_test.csv'\ndata = pd.read_csv(data_path)\n\n# Check the first few rows and the column names of the dataframe\nprint(data.head())\nprint(\"Columns in the DataFrame:\", data.columns.tolist())  # Print the actual column names\n\n# Ensure the dataframe contains the required columns\nexpected_columns = ['en', 'fr']\nassert all(col in data.columns for col in expected_columns), f\"CSV must contain {expected_columns} columns\"\n\n# Step 2: Convert the DataFrame to a TensorFlow Dataset\n# Create a TensorFlow dataset from the DataFrame\ntrain_dataset = tf.data.Dataset.from_tensor_slices((data['en'].values, data['fr'].values))\n\n# Print the first example to verify conversion\nfor english, french in train_dataset.take(1):\n    print(f'English: {english.numpy().decode(\"utf-8\")}, French: {french.numpy().decode(\"utf-8\")}')\n\n# Optional: Define constants for batch size and max length\nBATCH_SIZE = 64\nMAX_LENGTH = 40\n\n# Optional: Tokenization process\n# Tokenizer setup for input (English) and output (French)\ntokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (en.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\ntokenizer_fr = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n    (fr.numpy() for en, fr in train_dataset), target_vocab_size=2**13)\n\n# Encoding function\ndef encode(en_t, fr_t):\n    en_t = [tokenizer_en.vocab_size] + tokenizer_en.encode(en_t.numpy().decode('utf-8')) + [tokenizer_en.vocab_size + 1]\n    fr_t = [tokenizer_fr.vocab_size] + tokenizer_fr.encode(fr_t.numpy().decode('utf-8')) + [tokenizer_fr.vocab_size + 1]\n    return en_t, fr_t\n\ndef tf_encode(en_t, fr_t):\n    return tf.py_function(encode, [en_t, fr_t], [tf.int64, tf.int64])\n\n# Prepare the dataset with encoding\ntrain_dataset = train_dataset.map(tf_encode)\n\n# Filter sequences longer than MAX_LENGTH\ndef filter_max_length(en, fr, max_length=MAX_LENGTH):\n    return tf.logical_and(tf.size(en) <= max_length, tf.size(fr) <= max_length)\n\ntrain_dataset = train_dataset.filter(filter_max_length)\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(20000).padded_batch(BATCH_SIZE, padded_shapes=([None], [None]))\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n# Print the first training example after processing\nfor en, fr in train_dataset.take(1):\n    print(f'Encoded English: {en.numpy()}')\n    print(f'Encoded French: {fr.numpy()}')","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:30:54.522072Z","iopub.execute_input":"2024-10-18T07:30:54.522581Z","iopub.status.idle":"2024-10-18T07:31:47.910489Z","shell.execute_reply.started":"2024-10-18T07:30:54.522543Z","shell.execute_reply":"2024-10-18T07:31:47.909496Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"                                                  en  \\\n0              Spectacular Wingsuit Jump Over Bogota   \n1  Sportsman Jhonathan Florez jumped from a helic...   \n2  Wearing a wingsuit, he flew past over the famo...   \n3                           A black box in your car?   \n4  As America's road planners struggle to find th...   \n\n                                                  fr  \n0  Spectaculaire saut en \"wingsuit\" au-dessus de ...  \n1  Le sportif Jhonathan Florez a sautÃ© jeudi d'un...  \n2  EquipÃ© d'un wingsuit (une combinaison munie d'...  \n3               Une boÃ®te noire dans votre voitureÂ ?  \n4  Alors que les planificateurs du rÃ©seau routier...  \nColumns in the DataFrame: ['en', 'fr']\nEnglish: Spectacular Wingsuit Jump Over Bogota, French: Spectaculaire saut en \"wingsuit\" au-dessus de Bogota\nEncoded English: [[7639 2326   14 ... 7498 7429 7640]\n [7639   61    1 ...    0    0    0]\n [7639 6971 3706 ...    0    0    0]\n ...\n [7639   74  150 ...    0    0    0]\n [7639 1536   11 ...    0    0    0]\n [7639 2319 2407 ...    0    0    0]]\nEncoded French: [[8256 7523 8032 ... 7380 8046 8257]\n [8256  163    6 ...    0    0    0]\n [8256 7645 5985 ...    0    0    0]\n ...\n [8256  108  238 ...    0    0    0]\n [8256  367 5043 ...    0    0    0]\n [8256  422  966 ...    0    0    0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## WEEK 12: XI.DIALOGUE SYSTEM\n## a. BASIC RULE-BASED CHATBOT USING PYTHON NLTK","metadata":{}},{"cell_type":"code","source":"# Step 2: Import Libraries\nimport nltk\nfrom nltk.chat.util import Chat, reflections\n\n# Step 3: Define Rules (Predefined pairs)\npairs = [\n    (r\"my name is (.*)\", [\"Hello %1, How are you today?\"]),\n    (r\"hi|hey|hello\", [\"Hello\", \"Hey there\"]),\n    (r\"what is your name?\", [\"I am a bot created by [Your Name].\"]),\n    (r\"how are you?\", [\"I'm doing good. How about you?\"]),\n    (r\"sorry (.*)\", [\"No problem\", \"It's okay\", \"You don't need to be sorry\"]),\n    (r\"quit\", [\"Bye! Take care.\"])\n]\n\n# Step 4: Create the Chatbot\ndef chatbot():\n    print(\"Hi, I'm the chatbot you created. Type 'quit' to exit.\") \n    chat = Chat(pairs, reflections)\n    chat.converse()\n    \n# Step 5: Run the Chatbot\nif __name__ == \"__main__\":\n    chatbot()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:24:26.621475Z","iopub.execute_input":"2024-10-18T07:24:26.622013Z","iopub.status.idle":"2024-10-18T07:25:01.363584Z","shell.execute_reply.started":"2024-10-18T07:24:26.621970Z","shell.execute_reply":"2024-10-18T07:25:01.362598Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Hi, I'm the chatbot you created. Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> hello\n"},{"name":"stdout","text":"Hello\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> how are you?\n"},{"name":"stdout","text":"I'm doing good. How about you?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> quit\n"},{"name":"stdout","text":"Bye! Take care.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## b. BUILDING A CHATBOT USING SEQ2SEQ MODELS","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Step 1: Load and Preprocess the Dataset\ndef load_data(filepath):\n    try:\n        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n            lines = f.readlines()\n    except Exception as e:\n        print(f\"Error reading the file: {e}\")\n        return []\n\n    conversations = []\n    for line in lines:\n        line_parts = line.strip().split(' +++$+++ ')\n        if len(line_parts) == 5:\n            conversations.append(line_parts[4])  # Store only the dialogue part\n\n    print(f\"Loaded {len(conversations)} conversations.\")  # Debug info\n    return conversations\n\ndef create_pairs(conversations):\n    input_texts = []\n    target_texts = []\n\n    for i in range(len(conversations) - 1):\n        input_text = conversations[i]\n        target_text = conversations[i + 1]\n        target_text = '\\t' + target_text + '\\n'  # Add start and end tokens\n        input_texts.append(input_text)\n        target_texts.append(target_text)\n\n    print(f\"Created {len(input_texts)} input-target pairs.\")  # Debug info\n    return input_texts, target_texts\n\n# Load the dataset (replace with the correct path to movie_lines.txt)\nconversations = load_data('/kaggle/input/week12-dataset/movie_lines_rev 2.txt')  # Make sure this file exists\ninput_texts, target_texts = create_pairs(conversations)\n\n# Check if input_texts and target_texts are populated\nif not input_texts or not target_texts:\n    raise ValueError(\"No input or target texts were created. Please check the dataset.\")\n\n# Step 2: Tokenize and Pad the Data\n# Tokenize the input and output data\ninput_tokenizer = Tokenizer()\ntarget_tokenizer = Tokenizer()\n\ninput_tokenizer.fit_on_texts(input_texts)\ntarget_tokenizer.fit_on_texts(target_texts)\n\ninput_sequences = input_tokenizer.texts_to_sequences(input_texts)\ntarget_sequences = target_tokenizer.texts_to_sequences(target_texts)\n\n# Pad sequences to ensure uniform length\nmax_encoder_seq_length = max(len(seq) for seq in input_sequences) if input_sequences else 0\nmax_decoder_seq_length = max(len(seq) for seq in target_sequences) if target_sequences else 0\n\nencoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\ndecoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n\n# Prepare decoder output data\ndecoder_output_data = np.zeros((len(target_sequences), max_decoder_seq_length, len(target_tokenizer.word_index) + 1), dtype='float32')\n\nfor i, seq in enumerate(target_sequences):\n    for t, word_idx in enumerate(seq):\n        if t > 0:\n            decoder_output_data[i, t - 1, word_idx] = 1.0\n\n# Step 3: Build the Seq2Seq Model\nnum_encoder_tokens = len(input_tokenizer.word_index) + 1\nnum_decoder_tokens = len(target_tokenizer.word_index) + 1\n\n# Encoder\nencoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\nencoder_lstm = LSTM(256, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n\n# Save the encoder states to pass to the decoder\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Step 4: Compile and Train the Model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model (adjust epochs and batch size as needed)\nmodel.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=100)\n\n# Step 5: Inference Setup (for generating responses)\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# Step 6: Decode a Sequence (Generate a Response)\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate an empty target sequence with only the start token\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample the next token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n        decoded_sentence += sampled_char\n\n        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence and states\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n    return decoded_sentence.strip()  # Trim any extra whitespace\n\n# Step 7: Test the Chatbot\ndef chat():\n    print(\"Chatbot is ready! Type 'quit' to exit.\")\n    while True:\n        input_text = input(\"You: \")\n        if input_text.lower() == 'quit':\n            print(\"Exiting the chat. Goodbye!\")\n            break\n\n        input_sequence = input_tokenizer.texts_to_sequences([input_text])\n        input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n        response = decode_sequence(input_sequence)\n        print(f\"Bot: {response}\")\n\nif __name__ == \"__main__\":\n    chat()","metadata":{"execution":{"iopub.status.busy":"2024-10-18T07:32:06.204671Z","iopub.execute_input":"2024-10-18T07:32:06.205076Z","iopub.status.idle":"2024-10-18T07:34:31.802124Z","shell.execute_reply.started":"2024-10-18T07:32:06.205036Z","shell.execute_reply":"2024-10-18T07:34:31.800872Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Loaded 1153 conversations.\nCreated 1152 input-target pairs.\nEpoch 1/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 58ms/step - accuracy: 7.4284e-04 - loss: 0.4994\nEpoch 2/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.0015 - loss: 0.4596\nEpoch 3/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0030 - loss: 0.4852\nEpoch 4/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0026 - loss: 0.4477\nEpoch 5/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4324\nEpoch 6/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4557\nEpoch 7/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0029 - loss: 0.4740\nEpoch 8/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4519\nEpoch 9/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0031 - loss: 0.4715\nEpoch 10/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0025 - loss: 0.4649\nEpoch 11/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0029 - loss: 0.4628\nEpoch 12/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0029 - loss: 0.4588\nEpoch 13/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0029 - loss: 0.4620\nEpoch 14/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0029 - loss: 0.4814\nEpoch 15/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0027 - loss: 0.4618\nEpoch 16/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0027 - loss: 0.4702\nEpoch 17/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0027 - loss: 0.4535\nEpoch 18/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0027 - loss: 0.4537\nEpoch 19/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0026 - loss: 0.4569\nEpoch 20/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0026 - loss: 0.4389\nEpoch 21/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4776\nEpoch 22/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0030 - loss: 0.4768\nEpoch 23/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0026 - loss: 0.4458\nEpoch 24/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0025 - loss: 0.4624\nEpoch 25/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0029 - loss: 0.4588\nEpoch 26/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0027 - loss: 0.4549\nEpoch 27/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0029 - loss: 0.4875\nEpoch 28/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0027 - loss: 0.4637\nEpoch 29/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0031 - loss: 0.4436\nEpoch 30/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0026 - loss: 0.4652\nEpoch 31/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4676\nEpoch 32/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0026 - loss: 0.4398\nEpoch 33/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0026 - loss: 0.4494\nEpoch 34/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0029 - loss: 0.4544\nEpoch 35/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0028 - loss: 0.4533\nEpoch 36/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0025 - loss: 0.4438\nEpoch 37/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0025 - loss: 0.4252\nEpoch 38/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0026 - loss: 0.4616\nEpoch 39/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0025 - loss: 0.4663\nEpoch 40/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0029 - loss: 0.4202\nEpoch 41/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0027 - loss: 0.4490\nEpoch 42/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0028 - loss: 0.4417\nEpoch 43/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0029 - loss: 0.4835\nEpoch 44/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0027 - loss: 0.4601\nEpoch 45/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0024 - loss: 0.4411\nEpoch 46/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0025 - loss: 0.4369\nEpoch 47/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4699\nEpoch 48/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4836\nEpoch 49/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0025 - loss: 0.4398\nEpoch 50/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0028 - loss: 0.4612\nEpoch 51/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0027 - loss: 0.4515\nEpoch 52/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0026 - loss: 0.4550\nEpoch 53/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4461\nEpoch 54/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0028 - loss: 0.4820\nEpoch 55/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0029 - loss: 0.4654\nEpoch 56/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0026 - loss: 0.4503\nEpoch 57/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0027 - loss: 0.4548\nEpoch 58/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4570\nEpoch 59/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0030 - loss: 0.4569\nEpoch 60/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4582\nEpoch 61/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4516\nEpoch 62/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0030 - loss: 0.4574\nEpoch 63/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0029 - loss: 0.4457\nEpoch 64/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0025 - loss: 0.4798\nEpoch 65/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0026 - loss: 0.4394\nEpoch 66/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0028 - loss: 0.4646\nEpoch 67/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0029 - loss: 0.4665\nEpoch 68/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0027 - loss: 0.4590\nEpoch 69/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0026 - loss: 0.4682\nEpoch 70/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0026 - loss: 0.4620\nEpoch 71/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0026 - loss: 0.4556\nEpoch 72/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0026 - loss: 0.4576\nEpoch 73/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0028 - loss: 0.4642\nEpoch 74/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0027 - loss: 0.4442\nEpoch 75/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0029 - loss: 0.4766\nEpoch 76/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0030 - loss: 0.4795\nEpoch 77/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0024 - loss: 0.4364\nEpoch 78/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0032 - loss: 0.4560\nEpoch 79/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0023 - loss: 0.4371\nEpoch 80/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0027 - loss: 0.4581\nEpoch 81/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.0028 - loss: 0.4435\nEpoch 82/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0030 - loss: 0.4660\nEpoch 83/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0025 - loss: 0.4470\nEpoch 84/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0031 - loss: 0.4787\nEpoch 85/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0027 - loss: 0.4625\nEpoch 86/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0029 - loss: 0.4529\nEpoch 87/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0026 - loss: 0.4395\nEpoch 88/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0027 - loss: 0.4455\nEpoch 89/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0027 - loss: 0.4488\nEpoch 90/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0025 - loss: 0.4431\nEpoch 91/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0029 - loss: 0.4488\nEpoch 92/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0030 - loss: 0.4589\nEpoch 93/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0025 - loss: 0.4488\nEpoch 94/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0025 - loss: 0.4599\nEpoch 95/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0026 - loss: 0.4543\nEpoch 96/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - accuracy: 0.0028 - loss: 0.4502\nEpoch 97/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.0027 - loss: 0.4351\nEpoch 98/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.0028 - loss: 0.4574\nEpoch 99/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.0025 - loss: 0.4516\nEpoch 100/100\n\u001b[1m18/18\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.0026 - loss: 0.4509\nChatbot is ready! Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  hi\n"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 165\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[2], line 161\u001b[0m, in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m input_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([input_text])\n\u001b[1;32m    160\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_encoder_seq_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[2], line 127\u001b[0m, in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Generate an empty target sequence with only the start token\u001b[39;00m\n\u001b[1;32m    126\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 127\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    129\u001b[0m stop_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    130\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n","\u001b[0;31mKeyError\u001b[0m: '\\t'"],"ename":"KeyError","evalue":"'\\t'","output_type":"error"}]}]}